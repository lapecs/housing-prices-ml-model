{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e7199ea-39db-46b2-a613-db029c8bffb3",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea551741-858e-441d-875a-b3763b3892cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f19ed-ecc0-4d7c-8bed-57cb5831c64b",
   "metadata": {},
   "source": [
    "### Create a new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd53648-ff5f-4fd7-a2c3-0fd2e844dc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///C:/Users/lapec/mlruns/275983263025774426', creation_time=1744515082981, experiment_id='275983263025774426', last_update_time=1744515082981, lifecycle_stage='active', name='Housing Price Prediction', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set experiment name\n",
    "mlflow.set_experiment(\"Housing Price Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837d809d-a143-4bd6-81b2-7bf12724dfd8",
   "metadata": {},
   "source": [
    "### Create a directory to save training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad1ec6c0-58aa-4720-80b0-4b43a0281a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save our training data to\n",
    "def save_training_data(training_data, return_path = True):\n",
    "    \"\"\"Save training data to a local subdirectory.\n",
    "\n",
    "    Training data is typically saved to a central data warehouse. For \n",
    "    this project it will be saved to a local subdirectory.\n",
    "\n",
    "    Args:\n",
    "        training_data (pd.DataFrame): Pandas DataFrame containing training data\n",
    "        return_path (bool, optional): Will return the path to the saved training data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        str: Path to saved training data\n",
    "    \"\"\"\n",
    "    # Ensure subdirectory exists\n",
    "    os.makedirs(\"training_data\", exist_ok=True)\n",
    "\n",
    "    # Write out training data with run_id in file name\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    training_data.to_csv(f\"training_data/{run_id}-training_data.csv\", index=False)\n",
    "\n",
    "    if return_path:\n",
    "        training_data_path = os.path.abspath(f\"training_data/{run_id}-training_data.csv\")\n",
    "        return training_data_path\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c177203-e066-4808-b9b4-549a1acb403b",
   "metadata": {},
   "source": [
    "### Split the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bedd71-ae28-401e-bdbb-7850d5b02a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON lines file\n",
    "with open(r'G:\\My Drive\\MS Data Analytics\\Machine Learning Design\\housing_cleaned.json') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop([\"price\", \"date\"], axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Combine features and target into one DataFrame\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Create a Dataset object from the training DataFrame\n",
    "training_input = mlflow.data.from_pandas(training_data, targets=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2f420-311d-4ab4-ad8e-aeeb2b25787e",
   "metadata": {},
   "source": [
    "### Train a Regularized Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8d9fb7-f265-4341-94d3-2cad41db190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Identify any feature transformations\n",
    "feature_params = {\"preprocessing\": \"StandardScaler\"}\n",
    "\n",
    "model_params = {\n",
    "    \"alpha\": 0.5\n",
    "}\n",
    "\n",
    "# Train the Ridge Model\n",
    "reg = linear_model.Ridge(**model_params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = reg.predict(X_val_scaled)\n",
    "\n",
    "# Calculate error metrics\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# Assemble the metrics into a collection\n",
    "metrics = {\"mae\": mae, \"mape\": mape, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "run_name = \"Regularized Regression\"\n",
    "artifact_path = \"artifacts\"\n",
    "\n",
    "# Initiate the MLflow run context\n",
    "with mlflow.start_run(run_name = run_name) as run:\n",
    "    \n",
    "    # Log training data and preprocessing details\n",
    "    mlflow.log_input(training_input, context=\"training data\")\n",
    "    mlflow.log_params(feature_params)\n",
    "\n",
    "    # Log the parameters used for the model fit\n",
    "    mlflow.log_params(model_params)\n",
    "\n",
    "    # Log the error metrics that were calculated during validation\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Log an instance of the trained model for later use\n",
    "    mlflow.sklearn.log_model(sk_model=reg, input_example=X_val_scaled, artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81df1c-58d6-468e-ab13-d896eae9960f",
   "metadata": {},
   "source": [
    "### Train a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8712a6-d8a7-44f7-b33e-402b7d31e659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f335c95bf69469994f1c7f6f7618fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Non-scaled data will be used since Random Forest models do not require feature standardization\n",
    "feature_params = {\"preprocessing\": \"None\"}\n",
    "\n",
    "# Define Random Forest model parameters\n",
    "model_params = {\n",
    "    \"n_estimators\": 100,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 4,\n",
    "    \"bootstrap\": True,\n",
    "    \"oob_score\": False,\n",
    "    \"random_state\": 888\n",
    "}\n",
    "\n",
    "# Train a RandomForest model\n",
    "rf_model = RandomForestRegressor(**model_params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "y_pred = rf_model.predict(X_val)\n",
    "\n",
    "# Calculate error metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "# Assemble the metrics into a dictionary\n",
    "metrics = {\"rmse\": rmse, \"mae\": mae, \"mape\": mape, \"r2\": r2}\n",
    "metrics\n",
    "\n",
    "# Log experiment results and artifacts\n",
    "run_name = \"Random Forest\"\n",
    "artifact_path = \"artifacts\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Lot the dataset as an input\n",
    "    mlflow.log_input(training_input, context=\"training data\")\n",
    "\n",
    "    # Log the parameters used for the model fit\n",
    "    for param_name, param_value in model_params.items():\n",
    "        mlflow.log_param(param_name, param_value)\n",
    "\n",
    "    # Log the error metrics that were calculated during validation\n",
    "    mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Log an instance of the trained model for later use\n",
    "    mlflow.sklearn.log_model(sk_model=rf_model, input_example=X_val, artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae92cc-2a2a-4a69-9ad6-0a963768ea7d",
   "metadata": {},
   "source": [
    "### Random Forest Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "962b31ee-3661-4ec7-bcf6-d85b88bd8078",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927b85b07b474031a4e8e50741fd9290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define your param grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 6, 10, 20],\n",
    "    \"min_samples_split\": [5, 10, 15],\n",
    "    \"min_samples_leaf\": [2, 4, 6],\n",
    "    \"bootstrap\": [True, False]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Identify the best parameters and best estimators\n",
    "best_params = grid_search.best_params_\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the validation set using the best estimator\n",
    "y_pred_best = best_rf.predict(X_val)\n",
    "\n",
    "# Calculate error metrics\n",
    "mae_best = mean_absolute_error(y_val, y_pred_best)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))\n",
    "mape_best = mean_absolute_percentage_error(y_val, y_pred_best)\n",
    "r2_best = r2_score(y_val, y_pred_best)\n",
    "\n",
    "# Build a dictionary for the error metrics\n",
    "metrics_best = {\"mae\": mae_best, \"rmse\": rmse_best, \"mape\": mape_best, \"r2\": r2_best}\n",
    "\n",
    "# Log the experiment results and artifacts with MLflow\n",
    "run_name = \"Random Forest Hyperparameter Tuning\"\n",
    "artifact_path = \"artifacts\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "\n",
    "    # Lot the dataset and preprocessing details\n",
    "    mlflow.log_input(training_input, context=\"training data\")\n",
    "    mlflow.log_params(feature_params)\n",
    "\n",
    "    # Log the best parameters found by GridSearchCV\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Log the error metrics that were calculated during validation\n",
    "    mlflow.log_metrics(metrics_best)\n",
    "\n",
    "    # Log an instance of the best trained model for later use\n",
    "    mlflow.sklearn.log_model(sk_model=best_rf, input_example=X_val, artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf8eb4-c738-46b1-b490-3f7055ab3cb9",
   "metadata": {},
   "source": [
    "### Experiment using XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c7846a8-9a4f-402a-b7ac-ca20c52f2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:340228.50331\ttest-rmse:366587.84811\n",
      "[1]\ttrain-rmse:321025.93616\ttest-rmse:345919.11894\n",
      "[2]\ttrain-rmse:304190.41558\ttest-rmse:327832.41792\n",
      "[3]\ttrain-rmse:289514.74751\ttest-rmse:312971.56033\n",
      "[4]\ttrain-rmse:276322.04042\ttest-rmse:299211.73732\n",
      "[5]\ttrain-rmse:264256.36079\ttest-rmse:285930.95812\n",
      "[6]\ttrain-rmse:253752.63354\ttest-rmse:274602.34972\n",
      "[7]\ttrain-rmse:243930.81418\ttest-rmse:264283.18556\n",
      "[8]\ttrain-rmse:235125.62211\ttest-rmse:255326.45848\n",
      "[9]\ttrain-rmse:227482.77768\ttest-rmse:248434.39073\n",
      "[10]\ttrain-rmse:220510.65752\ttest-rmse:242344.37188\n",
      "[11]\ttrain-rmse:214249.02315\ttest-rmse:236150.90289\n",
      "[12]\ttrain-rmse:208310.79350\ttest-rmse:229850.49680\n",
      "[13]\ttrain-rmse:202934.45141\ttest-rmse:223531.66121\n",
      "[14]\ttrain-rmse:197940.46421\ttest-rmse:217710.85242\n",
      "[15]\ttrain-rmse:192987.29563\ttest-rmse:212188.16277\n",
      "[16]\ttrain-rmse:188958.61531\ttest-rmse:208471.56680\n",
      "[17]\ttrain-rmse:185028.07251\ttest-rmse:204470.44486\n",
      "[18]\ttrain-rmse:181799.24682\ttest-rmse:202645.80537\n",
      "[19]\ttrain-rmse:178735.66990\ttest-rmse:199829.87613\n",
      "[20]\ttrain-rmse:175954.08289\ttest-rmse:197499.81409\n",
      "[21]\ttrain-rmse:172780.94432\ttest-rmse:194100.52036\n",
      "[22]\ttrain-rmse:170451.96673\ttest-rmse:193806.63074\n",
      "[23]\ttrain-rmse:167507.52768\ttest-rmse:191723.51852\n",
      "[24]\ttrain-rmse:164803.77021\ttest-rmse:189398.51412\n",
      "[25]\ttrain-rmse:163004.44044\ttest-rmse:188129.99293\n",
      "[26]\ttrain-rmse:160526.28798\ttest-rmse:187104.62996\n",
      "[27]\ttrain-rmse:158793.19242\ttest-rmse:186183.77380\n",
      "[28]\ttrain-rmse:156503.35670\ttest-rmse:184294.70653\n",
      "[29]\ttrain-rmse:154958.55613\ttest-rmse:183096.52075\n",
      "[30]\ttrain-rmse:153144.35661\ttest-rmse:182228.68214\n",
      "[31]\ttrain-rmse:151457.29726\ttest-rmse:181168.83097\n",
      "[32]\ttrain-rmse:149848.62406\ttest-rmse:179387.76117\n",
      "[33]\ttrain-rmse:148646.41129\ttest-rmse:178829.38762\n",
      "[34]\ttrain-rmse:147566.43714\ttest-rmse:178598.48798\n",
      "[35]\ttrain-rmse:146210.42987\ttest-rmse:177719.11797\n",
      "[36]\ttrain-rmse:144879.89121\ttest-rmse:176551.75898\n",
      "[37]\ttrain-rmse:143993.84990\ttest-rmse:176356.68128\n",
      "[38]\ttrain-rmse:142856.28195\ttest-rmse:175703.79708\n",
      "[39]\ttrain-rmse:141884.30806\ttest-rmse:174938.39099\n",
      "[40]\ttrain-rmse:140689.61831\ttest-rmse:173870.06347\n",
      "[41]\ttrain-rmse:140051.88446\ttest-rmse:174155.96587\n",
      "[42]\ttrain-rmse:138957.47875\ttest-rmse:173631.00727\n",
      "[43]\ttrain-rmse:138275.59271\ttest-rmse:173049.98596\n",
      "[44]\ttrain-rmse:137116.55088\ttest-rmse:171807.01159\n",
      "[45]\ttrain-rmse:136451.94464\ttest-rmse:171309.57316\n",
      "[46]\ttrain-rmse:135907.76260\ttest-rmse:171619.79596\n",
      "[47]\ttrain-rmse:135069.47000\ttest-rmse:170898.32275\n",
      "[48]\ttrain-rmse:134275.36936\ttest-rmse:170501.25536\n",
      "[49]\ttrain-rmse:133771.22068\ttest-rmse:170052.80378\n",
      "[50]\ttrain-rmse:133290.92041\ttest-rmse:169800.25743\n",
      "[51]\ttrain-rmse:132521.45379\ttest-rmse:169110.60061\n",
      "[52]\ttrain-rmse:131785.67845\ttest-rmse:168485.97429\n",
      "[53]\ttrain-rmse:131381.49740\ttest-rmse:168548.81007\n",
      "[54]\ttrain-rmse:130883.84407\ttest-rmse:168555.06169\n",
      "[55]\ttrain-rmse:130268.85149\ttest-rmse:167797.68818\n",
      "[56]\ttrain-rmse:129640.32965\ttest-rmse:167657.30173\n",
      "[57]\ttrain-rmse:128976.18739\ttest-rmse:167026.61265\n",
      "[58]\ttrain-rmse:128377.88494\ttest-rmse:166504.71539\n",
      "[59]\ttrain-rmse:127457.94790\ttest-rmse:165661.19244\n",
      "[60]\ttrain-rmse:127042.42717\ttest-rmse:165343.49694\n",
      "[61]\ttrain-rmse:126603.99992\ttest-rmse:164804.91405\n",
      "[62]\ttrain-rmse:126256.29253\ttest-rmse:165230.86088\n",
      "[63]\ttrain-rmse:125893.69525\ttest-rmse:164789.31882\n",
      "[64]\ttrain-rmse:125659.01867\ttest-rmse:164688.03194\n",
      "[65]\ttrain-rmse:125232.71303\ttest-rmse:164322.05729\n",
      "[66]\ttrain-rmse:124488.47199\ttest-rmse:163563.89112\n",
      "[67]\ttrain-rmse:124231.77073\ttest-rmse:163376.62723\n",
      "[68]\ttrain-rmse:123884.53742\ttest-rmse:163346.99877\n",
      "[69]\ttrain-rmse:123683.87477\ttest-rmse:163242.05444\n",
      "[70]\ttrain-rmse:123363.05922\ttest-rmse:163232.38898\n",
      "[71]\ttrain-rmse:122994.25852\ttest-rmse:162880.91174\n",
      "[72]\ttrain-rmse:122682.95502\ttest-rmse:162675.60742\n",
      "[73]\ttrain-rmse:122331.57485\ttest-rmse:162217.81873\n",
      "[74]\ttrain-rmse:122078.63022\ttest-rmse:162016.67693\n",
      "[75]\ttrain-rmse:121954.72930\ttest-rmse:162173.89243\n",
      "[76]\ttrain-rmse:121746.84290\ttest-rmse:162482.87109\n",
      "[77]\ttrain-rmse:121429.98100\ttest-rmse:162211.11540\n",
      "[78]\ttrain-rmse:120845.30492\ttest-rmse:161571.66237\n",
      "[79]\ttrain-rmse:120652.81427\ttest-rmse:161540.74108\n",
      "[80]\ttrain-rmse:120377.35048\ttest-rmse:161328.89292\n",
      "[81]\ttrain-rmse:120220.35457\ttest-rmse:161146.55085\n",
      "[82]\ttrain-rmse:120084.35900\ttest-rmse:160940.11583\n",
      "[83]\ttrain-rmse:119823.33694\ttest-rmse:160711.60254\n",
      "[84]\ttrain-rmse:119644.32804\ttest-rmse:160603.38754\n",
      "[85]\ttrain-rmse:119358.56857\ttest-rmse:160438.89086\n",
      "[86]\ttrain-rmse:119216.31095\ttest-rmse:160373.02509\n",
      "[87]\ttrain-rmse:119018.18250\ttest-rmse:160463.76514\n",
      "[88]\ttrain-rmse:118769.80903\ttest-rmse:160295.87627\n",
      "[89]\ttrain-rmse:118590.44783\ttest-rmse:160170.72289\n",
      "[90]\ttrain-rmse:118437.71990\ttest-rmse:160367.05719\n",
      "[91]\ttrain-rmse:118224.23737\ttest-rmse:160308.66227\n",
      "[92]\ttrain-rmse:118064.05181\ttest-rmse:160257.22155\n",
      "[93]\ttrain-rmse:117944.49170\ttest-rmse:160528.76513\n",
      "[94]\ttrain-rmse:117717.06302\ttest-rmse:160355.75972\n",
      "[95]\ttrain-rmse:117615.52047\ttest-rmse:160281.39220\n",
      "[96]\ttrain-rmse:117419.61051\ttest-rmse:160128.38238\n",
      "[97]\ttrain-rmse:117295.45798\ttest-rmse:160086.82099\n",
      "[98]\ttrain-rmse:117173.70499\ttest-rmse:160278.49489\n",
      "[99]\ttrain-rmse:117032.20896\ttest-rmse:160260.14549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lapec\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [18:47:31] WARNING: C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\c_api\\c_api.cc:1374: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "2025/04/12 18:47:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration: 97\n",
      "Best MAE: 160086.82098924066 (based on test set)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import mlflow.xgboost\n",
    "\n",
    "# Prepare data for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "# Set hyperparameters for XGBoost\n",
    "params = {\"objective\": \"reg:squarederror\",\n",
    "          \"max_depth\": 3, \n",
    "          \"eta\": 0.1, \n",
    "          \"eval_metric\": \"rmse\" # Default evaluation metric\n",
    "         }\n",
    "\n",
    "# Train the model\n",
    "evals = [(dtrain, 'train'), (dtest, 'test')]\n",
    "model = xgb.train(\n",
    "    params, dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals = evals,\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "# Log the experiment results and artifacts with MLflow\n",
    "run_name = \"XGBoost\"\n",
    "artifact_path = \"artifacts\"\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14dcb8b-e5a8-438a-b731-8dc1177c44e1",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)\n",
    "KNN can be used for regression, where the output value for a test point is the average of the nearest neighbors. It can be a good option if the dataset has local patterns that are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc514e65-5b2a-4fe8-ad75-629ce7db4eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/12 18:50:29 INFO mlflow.tracking.fluent: Experiment with name 'housing-price-kneighbors' does not exist. Creating a new experiment.\n",
      "2025/04/12 18:50:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Initialize the model\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"housing-price-kneighbors\")\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_params({'n_neighbors': 5})\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b8a9a-6ca7-471a-a6d7-9335b87fb50e",
   "metadata": {},
   "source": [
    "### Training for Neural Network\n",
    "Need to load NN specific dataset in and train the neural network on it - using learned embeddings with an embedding layer for Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99de622-d69f-4770-a78f-10d1694f23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Assume this is your preprocessed DataFrame (from earlier)\n",
    "df = pd.read_csv(\"/Users/Ethanbayer/Documents/Group Project ML/housing_for_nn.csv\"\n",
    "\n",
    "# Define inputs\n",
    "target_col = 'price'\n",
    "ordinal_cols = ['condition', 'grade', 'view', 'floors']\n",
    "numeric_cols = [\n",
    "    'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n",
    "    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',\n",
    "    'lat', 'long', 'sqft_living15', 'sqft_lot15'\n",
    "]\n",
    "zipcode_col = 'zipcode'\n",
    "\n",
    "# Separate features and target\n",
    "X = df[numeric_cols + ordinal_cols + [zipcode_col]]\n",
    "y = df[target_col].values\n",
    "\n",
    "# Split into train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate numeric + ordinal and zipcode\n",
    "X_train_numeric = X_train[numeric_cols + ordinal_cols].values\n",
    "X_train_zip = X_train[zipcode_col].values\n",
    "X_val_numeric = X_val[numeric_cols + ordinal_cols].values\n",
    "X_val_zip = X_val[zipcode_col].values\n",
    "\n",
    "# Normalize non-zipcode features\n",
    "scaler = StandardScaler()\n",
    "X_train_numeric = scaler.fit_transform(X_train_numeric)\n",
    "X_val_numeric = scaler.transform(X_val_numeric)\n",
    "\n",
    "# Neural Net parameters\n",
    "embedding_dim = 4\n",
    "n_zipcodes = df[zipcode_col].nunique()\n",
    "hidden_units = [64, 32]\n",
    "\n",
    "# Build model\n",
    "input_numeric = Input(shape=(X_train_numeric.shape[1],), name='numeric')\n",
    "input_zipcode = Input(shape=(1,), dtype='int32', name='zipcode')\n",
    "zipcode_embedding = layers.Embedding(input_dim=n_zipcodes + 1, output_dim=embedding_dim)(input_zipcode)\n",
    "zipcode_embedding = layers.Flatten()(zipcode_embedding)\n",
    "\n",
    "x = layers.Concatenate()([input_numeric, zipcode_embedding])\n",
    "for units in hidden_units:\n",
    "    x = layers.Dense(units, activation='relu')(x)\n",
    "output = layers.Dense(1)(x)\n",
    "\n",
    "model = Model(inputs=[input_numeric, input_zipcode], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    {'numeric': X_train_numeric, 'zipcode': X_train_zip},\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=({'numeric': X_val_numeric, 'zipcode': X_val_zip}, y_val),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and compute metrics\n",
    "y_pred = model.predict({'numeric': X_val_numeric, 'zipcode': X_val_zip}).flatten()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "metrics = {\"rmse\": rmse, \"mae\": mae, \"mape\": mape, \"r2\": r2}\n",
    "params = {\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"hidden_layers\": hidden_units,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 32\n",
    "}\n",
    "\n",
    "# MLflow logging\n",
    "run_name = \"Neural Network with Zipcode Embedding\"\n",
    "artifact_path = \"nn_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    # Log input metadata (optional)\n",
    "    # mlflow.log_input(X_train, context=\"training data\")  # uncomment if using MLflow 2.0+ with data tracking\n",
    "    \n",
    "    for name, val in params.items():\n",
    "        mlflow.log_param(name, val)\n",
    "    \n",
    "    mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Save model using MLflow\n",
    "    mlflow.tensorflow.log_model(tf_model=model, artifact_path=artifact_path)\n",
    "\n",
    "# View metrics\n",
    "metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
