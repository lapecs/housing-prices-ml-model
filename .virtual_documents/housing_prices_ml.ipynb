


import mlflow
import numpy as np
import pandas as pd
import os
import json
from sklearn import linear_model
from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score
from sklearn.preprocessing import StandardScaler
import mlflow.sklearn
from mlflow.models.signature import infer_signature





# Set experiment name
mlflow.set_experiment("Housing Price Prediction")





# Create a directory to save our training data to
def save_training_data(training_data, return_path = True):
    """Save training data to a local subdirectory.

    Training data is typically saved to a central data warehouse. For 
    this project it will be saved to a local subdirectory.

    Args:
        training_data (pd.DataFrame): Pandas DataFrame containing training data
        return_path (bool, optional): Will return the path to the saved training data. Defaults to True.

    Returns:
        str: Path to saved training data
    """
    # Ensure subdirectory exists
    os.makedirs("training_data", exist_ok=True)

    # Write out training data with run_id in file name
    run_id = mlflow.active_run().info.run_id
    training_data.to_csv(f"training_data/{run_id}-training_data.csv", index=False)

    if return_path:
        training_data_path = os.path.abspath(f"training_data/{run_id}-training_data.csv")
        return training_data_path
    else:
        return None





# Load the JSON lines file
with open('housing_cleaned.json') as f:
    data = [json.loads(line) for line in f]

df = pd.DataFrame(data)

# Define features and target
X = df.drop(["price", "date"], axis=1)
y = df["price"]

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)

# Combine features and target into one DataFrame
training_data = pd.concat([X_train, y_train], axis=1)

# Create a Dataset object from the training DataFrame
training_input = mlflow.data.from_pandas(training_data, targets="price")





# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)

# Identify any feature transformations
feature_params = {"preprocessing": "StandardScaler"}

model_params = {
    "alpha": 0.5
}

# Train the Ridge Model
reg = linear_model.Ridge(**model_params)

# Fit the model on the training data
reg.fit(X_train_scaled, y_train)

# Predict on the validation set
y_pred = reg.predict(X_val_scaled)

# Calculate error metrics
mae = mean_absolute_error(y_val, y_pred)
mape = mean_absolute_percentage_error(y_val, y_pred)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
r2 = r2_score(y_val, y_pred)

# Assemble the metrics into a collection
metrics = {"mae": mae, "mape": mape, "rmse": rmse, "r2": r2}




# Log experiment results and artifacts in MLflow
run_name = "Regularized Regression"
artifact_path = "artifacts"

# Initiate the MLflow run context
with mlflow.start_run(run_name = run_name) as run:
    
    # Log training data and preprocessing details
    mlflow.log_input(training_input, context="training data")
    mlflow.log_params(feature_params)

    # Log the parameters used for the model fit
    mlflow.log_params(model_params)

    # Log the error metrics that were calculated during validation
    mlflow.log_metrics(metrics)

    # Log an instance of the trained model for later use
    mlflow.sklearn.log_model(sk_model=reg, input_example=X_val_scaled, artifact_path=artifact_path)





# Non-scaled data will be used since Random Forest models do not require feature standardization
feature_params = {"preprocessing": "None"}

# Define Random Forest model parameters
model_params = {
    "n_estimators": 200,
    "max_depth": 6,
    "min_samples_split": 10,
    "min_samples_leaf": 4,
    "bootstrap": True,
    "oob_score": False,
    "random_state": 888
}

# Train a RandomForest model
rf_model = RandomForestRegressor(**model_params)

# Fit the model on the training data
rf_model.fit(X_train, y_train)

# Evaluate the Random Forest model
y_pred = rf_model.predict(X_val)

# Calculate error metrics
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
mae = mean_absolute_error(y_val, y_pred)
mape = mean_absolute_percentage_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

# Assemble the metrics into a dictionary
metrics = {"rmse": rmse, "mae": mae, "mape": mape, "r2": r2}
metrics



# Log experiment results and artifacts in MLflow
run_name = "Random Forest"
artifact_path = "artifacts"

with mlflow.start_run(run_name=run_name) as run:

    # Lot the dataset as an input
    mlflow.log_input(training_input, context="training data")

    # Log the parameters used for the model fit
    for param_name, param_value in model_params.items():
        mlflow.log_param(param_name, param_value)

    # Log the error metrics that were calculated during validation
    mlflow.log_metrics(metrics)

    # Log an instance of the trained model for later use
    mlflow.sklearn.log_model(sk_model=rf_model, input_example=X_val, artifact_path=artifact_path)





# Define your param grid
param_grid = {
    "n_estimators": [100, 200],
    "max_depth": [None, 6, 20],
    "min_samples_split": [5, 15],
    "min_samples_leaf": [2, 4, 6],
    "bootstrap": [True, False]
}

# Initialize the GridSearchCV object
grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Identify the best parameters and best estimators
best_params = grid_search.best_params_
best_rf = grid_search.best_estimator_

# Predict on the validation set using the best estimator
y_pred_best = best_rf.predict(X_val)

# Calculate error metrics
mae_best = mean_absolute_error(y_val, y_pred_best)
rmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))
mape_best = mean_absolute_percentage_error(y_val, y_pred_best)
r2_best = r2_score(y_val, y_pred_best)

# Build a dictionary for the error metrics
metrics_best = {"mae": mae_best, "rmse": rmse_best, "mape": mape_best, "r2": r2_best}



# Log the experiment results and artifacts with MLflow
run_name = "Random Forest Hyperparameter Tuning"
artifact_path = "artifacts"

with mlflow.start_run(run_name=run_name) as run:

    # Lot the dataset and preprocessing details
    mlflow.log_input(training_input, context="training data")
    mlflow.log_params(best_params)

    # Log the best parameters found by GridSearchCV
    mlflow.log_params(best_params)

    # Log the error metrics that were calculated during validation
    mlflow.log_metrics(metrics_best)

    # Log an instance of the best trained model for later use
    mlflow.sklearn.log_model(sk_model=best_rf, input_example=X_val, artifact_path=artifact_path)
    
    print("Random Forest Hyperparamter Tuning model logged in MLflow!")






import xgboost as xgb
import mlflow.xgboost

# Prepare data for XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_val, label=y_val)

# Set hyperparameters for XGBoost
feature_params = {"objective": "reg:squarederror",
                  "max_depth": 3, 
                  "eta": 0.1, 
                  "eval_metric": "rmse" # Default evaluation metric
                 }

# Train the model
evals = [(dtrain, 'train'), (dtest, 'test')]
model = xgb.train(
    feature_params, dtrain,
    num_boost_round=100,
    evals = evals,
    early_stopping_rounds=10
)

# Predict on the validation set using the best estimator
y_pred_best = model.predict(dtest)

# Calculate error metrics
mae_best = mean_absolute_error(y_val, y_pred_best)
rmse_best = np.sqrt(mean_squared_error(y_val, y_pred_best))
mape_best = mean_absolute_percentage_error(y_val, y_pred_best)
r2_best = r2_score(y_val, y_pred_best)

# Build a dictionary for the error metrics
metrics_best = {"mae": mae_best, "rmse": rmse_best, "mape": mape_best, "r2": r2_best}



# Log the experiment results and artifacts with MLflow
from mlflow.models.signature import infer_signature

run_name = "XGBoost"

# Start an MLflow run
with mlflow.start_run(run_name=run_name) as run:

    # Log model parameters
    mlflow.log_params(feature_params)

    # Log metrics
    mlflow.log_metrics(metrics_best)

    # Convert all integer columns to float before logging
    X_val_fixed = X_val.astype({col: 'float64' for col in X_val.select_dtypes('int').columns})
    
    # Infer signature
    signature = infer_signature(X_val_fixed, y_pred_best)

    # Prepare input example (first row of X_val)
    input_example = X_val_fixed.iloc[:1] if hasattr(X_val, "iloc") else X_val[:1]

    # Log the model
    mlflow.xgboost.log_model(
        xgb_model=model,
        artifact_path="model",
        signature=signature,
        input_example=input_example
    )

    print("XGBoost model logged in MLflow!")





# from sklearn.neighbors import KNeighborsRegressor

# # Initialize the model
# model = KNeighborsRegressor(n_neighbors=5)

# # Train the model
# model.fit(X_train, y_train)

# # Set experiment
# mlflow.set_experiment("housing-price-kneighbors")

# # Log the model to MLflow
# with mlflow.start_run():
#     mlflow.log_params({'n_neighbors': 5})
#     mlflow.sklearn.log_model(model, "model")






import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, Input, Model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler
import mlflow
import mlflow.tensorflow

# Assume this is your preprocessed DataFrame (from earlier)
df = pd.read_csv("housing_for_nn.csv")

# Define inputs
target_col = 'price'
ordinal_cols = ['condition', 'grade', 'view', 'floors']
numeric_cols = [
    'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',
    'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated',
    'lat', 'long', 'sqft_living15', 'sqft_lot15'
]
zipcode_col = 'zipcode'

# Separate features and target
X = df[numeric_cols + ordinal_cols + [zipcode_col]]
y = df[target_col].values

# Split into train/val
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Separate numeric + ordinal and zipcode
X_train_numeric = X_train[numeric_cols + ordinal_cols].values
X_train_zip = X_train[zipcode_col].values
X_val_numeric = X_val[numeric_cols + ordinal_cols].values
X_val_zip = X_val[zipcode_col].values

# Normalize non-zipcode features
scaler = StandardScaler()
X_train_numeric = scaler.fit_transform(X_train_numeric)
X_val_numeric = scaler.transform(X_val_numeric)

# Build a lookup for zipcodes â†’ integer index
unique_zipcodes = df[zipcode_col].unique()
zipcode_lookup = {zip_code: idx for idx, zip_code in enumerate(unique_zipcodes)}

# Map zipcodes to index values
X_train_zip = X_train[zipcode_col].map(zipcode_lookup).values
X_val_zip = X_val[zipcode_col].map(zipcode_lookup).values

# Neural Net parameters
embedding_dim = 4
n_zipcodes = len(zipcode_lookup)
hidden_units = [64, 32]

# Build model
input_numeric = Input(shape=(X_train_numeric.shape[1],), name='numeric')
input_zipcode = Input(shape=(1,), dtype='int32', name='zipcode')
zipcode_embedding = layers.Embedding(input_dim=n_zipcodes + 1, output_dim=embedding_dim)(input_zipcode)
zipcode_embedding = layers.Flatten()(zipcode_embedding)

x = layers.Concatenate()([input_numeric, zipcode_embedding])
for units in hidden_units:
    x = layers.Dense(units, activation='relu')(x)
output = layers.Dense(1)(x)

model = Model(inputs=[input_numeric, input_zipcode], outputs=output)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
model.fit(
    {'numeric': X_train_numeric, 'zipcode': X_train_zip},
    y_train,
    epochs=50,
    batch_size=32,
    validation_data=({'numeric': X_val_numeric, 'zipcode': X_val_zip}, y_val),
    verbose=1
)

# Predict and compute metrics
y_pred = model.predict({'numeric': X_val_numeric, 'zipcode': X_val_zip}).flatten()

rmse = np.sqrt(mean_squared_error(y_val, y_pred))
mae = mean_absolute_error(y_val, y_pred)
mape = mean_absolute_percentage_error(y_val, y_pred)
r2 = r2_score(y_val, y_pred)

metrics = {"rmse": rmse, "mae": mae, "mape": mape, "r2": r2}
params = {
    "embedding_dim": embedding_dim,
    "hidden_layers": hidden_units,
    "optimizer": "adam",
    "epochs": 50,
    "batch_size": 32
}


# MLflow logging
run_name = "Neural Network with Zipcode Embedding"
artifact_path = "nn_model"

# Since there are two inputs for this model, the input_example will need to be defined to include both
numeric_example = X_val_numeric[:1]  # shape (1, num_features)
zipcode_example = X_val_zip[:1]      # shape (1,)

input_example = {
    "numeric": numeric_example,
    "zipcode": zipcode_example
}

with mlflow.start_run(run_name=run_name) as run:
    # Log input metadata (optional)
    # mlflow.log_input(X_train, context="training data")  # uncomment if using MLflow 2.0+ with data tracking
    
    for name, val in params.items():
        mlflow.log_param(name, val)
    
    mlflow.log_metrics(metrics)
    
    # Save model using MLflow
    mlflow.tensorflow.log_model(model=model, 
                                input_example=input_example, 
                                artifact_path=artifact_path,
                                signature=mlflow.models.infer_signature(
                                    {"numeric": X_val_numeric, "zipcode": X_val_zip})
                                )



